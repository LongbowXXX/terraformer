# Copilot Custom Prompting Techniques

This document summarizes effective techniques for creating custom prompts for GitHub Copilot in VS Code.

## Task Management with `#todos`

### Problem

When a custom prompt requires performing multiple complex operations (e.g., analyzing context, generating multiple files, validating links), Copilot may occasionally skip steps or lose track of progress, leading to incomplete results.

### Solution

Enforce structured execution by explicitly registering tasks using the `#todos` tool at the beginning of the prompt and requiring a final verification step.

### Implementation Steps

1.  **Task Initialization**:
    At the very beginning of the prompt (after the role definition), instruct Copilot to immediately use the `#todos` tool to register all high-level tasks.

    > [!IMPORTANT]
    > Ensure that the TODO items and the work Steps match. If they do not match, the AI may make mistakes when checking progress.

2.  **Final Verification**:
    Add a "Final Check" section at the end of the prompt that requires Copilot to confirm all registered todos are completed.

### Example Template

```markdown
# Role: [Role Name]

...

## ðŸ“‹ Task Initialization

**IMMEDIATELY** use the `#todos` tool to register the following tasks to track your progress:

1.  **Step 1**: [Description of step 1]
2.  **Step 2**: [Description of step 2]
3.  **Validation**: Perform checks.
4.  **Final Check**: Review the "Final Check" section.

... [Rest of the prompt] ...

## âœ… Final Check

**Before finishing, confirm:**

- [ ] All todos are marked as completed.
- [ ] All requirements are met.
```

### Benefits

- **Visibility**: Users can see the planned tasks in the "Todos" view.
- **Completeness**: Reduces the risk of missing steps in complex workflows.
- **Self-Correction**: Encourages the model to review its own work against the checklist.

## Knowledge Retrieval for Latest Specifications

### Problem

Copilot's training data has a cutoff, so it may not know about the latest features, file structures, or tools available in VS Code Copilot (e.g., new prompt file formats, new chat tools).

### Solution

Explicitly instruct Copilot to fetch the latest documentation from official URLs at the beginning of the prompt before performing any generation tasks.

### Implementation Steps

1.  **Identify Critical URLs**:
    Determine which documentation pages contain the specifications relevant to your prompt (e.g., prompt files, custom agents, tools).

2.  **Add Prerequisite Section**:
    Add a section (e.g., "PREREQUISITE: Knowledge Retrieval") that forces Copilot to read these URLs.

### Example Template

```markdown
## ðŸš¨ PREREQUISITE: Knowledge Retrieval

**Before generating or updating, you MUST:**

1.  **Fetch Latest Docs**:
    - `https://code.visualstudio.com/docs/copilot/customization/prompt-files` (for prompt file structure).
    - `https://code.visualstudio.com/docs/copilot/reference/copilot-vscode-features#_chat-tools` (for available tools).
    - `https://code.visualstudio.com/docs/copilot/customization/custom-agents` (for custom agent structure).
```

### Benefits

- **Accuracy**: Ensures generated files comply with the latest schemas and standards.
- **Capability**: Enables the use of the newest tools and features that the model might not inherently know.

## Transparency & Hallucination Management

### Problem

Users may unknowingly treat AI-generated content as "Gold Standard," leading to critical errors if hallucinations go unnoticed. Conversely, users may spend excessive time correcting minor, non-critical AI mistakes, negating the efficiency gains of using AI.

### Solution

Enforce a "Transparency" policy by requiring AI agents to embed a specific generation tag at the top of all created documents. This explicitly labels the content as an AI draft.

### Implementation Steps

1.  **Define the Tag**:
    Standardize a comment format (e.g., `<!-- This document is generated by @AgentName -->`).

2.  **Instruction Injection**:
    In the skill or agent definition (`.prompt.md` or `.agent.md`), explicitly instruct the model: _"You MUST include the following tag at the top of the file..."_

3.  **Adopt "Good Enough" Philosophy**:
    Encourage users (via documentation like `PROJECT_CHARTER.md` or `coding-conventions.md`) to treat tagged documents as "Junior Developer Drafts"â€”verifying key logic but accepting minor formatting or phrasing quirks.

### Example Template

````markdown
# Output Format

Generate the file with the following header:

```markdown
<!-- This document is generated and updated by .github/prompts/my-skill.prompt.md -->

# [Title]

...
```
````

### Benefits

- **Safety**: Explicit warning prompts necessary verification.
- **Efficiency**: Shifts user mindset from "Perfectionism" to "Validation," speeding up reviews.
- **Traceability**: Identifies which agent or prompt created the file.

## Stop and Ask (Circuit Breaker)

### Problem

Agents can sometimes get stuck in a loop of trying seeing errors, trying a guess fix, seeing errors again, and repeating. Or they may proceed with a fix based on a weak assumption that turns out to be wrong, wasting time and potentially damaging code.

### Solution

Explicitly instruct the agent to **STOP** and **ASK** the user if it cannot verify the root cause or if it has failed multiple times.

### Implementation Steps

1.  **Define Stop Conditions**:
    Decide on a reasonable number of attempts (e.g., 3 tries) or specific uncertainty triggers (e.g., "cannot reproduce").

2.  **Add "Constraints" or "Stop Condition" Section**:
    In the prompt or agent template, add a clear rule that overrides the "fix it" mandate.

### Example Template

```markdown
## â›” Stop Condition

If you cannot identify the root cause after **3 attempts** or if the fix requires making unverified assumptions:

1.  **STOP** execution.
2.  **Report** your findings and what you have tried so far.
3.  **Ask** the user for further guidance or more information.
```

### Benefits

- **Efficiency**: Prevents "flailing" where the agent burns tokens and time on bad paths.
- **Safety**: Reduces the chance of hallucinated "fixes" breaking things further.
- **Collaboration**: Brings the human back in the loop exactly when needed.

## Environment-Agnostic Tool Support

### Problem

Official GitHub Copilot best practices suggest explicitly defining the `tools` available to an agent or prompt. However, this approach has significant downsides when generating prompts automatically:

- **Environment Variability**: Developers' environments vary significantly (different Extensions, MCP servers). Hardcoding tools inadvertently disables access to context-specific tools that the user might have installed.
- **Hallucinations**: AI models frequently hallucinate tool names that do not exist explicitly or use incorrect names, leading to broken prompts or errors.

If an AI-generated prompt explicitly hardcodes a list of `tools`, it limits flexibility and increases the risk of including invalid tools.

### Solution

**Deliberately ignore** the best practice of explicit tool specification when generating new custom prompts or agents. Instead, omit the `tools` property entirely. This allows the default behavior to take over, which generally permits access to all tools available in the current context (including those from Extensions and MCP servers).

**Exception**: If you are basing a new prompt on a specific existing reference (e.g., copying a strict agent definition), it is acceptable to copy the `tools` list to maintain the intended behavior of that reference.

### Implementation Steps

1.  **Omit `tools` Property**:
    When creating `.prompt.md` or `.agent.md` files, do not generate a `tools: [...]` section.

2.  **User Delegation**:
    If specific tools must be restricted or strictly managed, leave that configuration to the human user.

### Example Template

**Avoid this (unless copying strict reference):**

```markdown
---
name: My Strict Agent
description: An agent with hardcoded tools
tools: ["search", "fetch"]
---
```

**Prefer this (Environment-Agnostic):**

```markdown
---
name: My Flexible Agent
description: An agent that can use whatever tools the user has installed.
---
```

### Benefits

- **Flexibility**: Automatically supports new tools added by extensions or MCP servers without code changes.
- **Portability**: The prompt works across different developer setups without breaking due to missing or extra tools.
- **Power**: Allows the agent to leverage the full capabilities of the user's IDE.

## Iterative Changes for High-Impact Configs

### Problem

When generating configuration files (like `.vscode/tasks.json` or `settings.json`) or managing lists (like extensions), generating the entire file at once forces the user to review a large block of changes. This often leads to "information overload," making it difficult for the user to accept partial suggestions or verify correctness.

### Solution

Adopt an **Iterative Proposal Loop**. Instead of generating the final file immediately, instruct the agent to propose changes **one by one** (or in small logical groups), explain the rationale, and ask for user confirmation before proceeding to the next item.

### Implementation Steps

1.  **Define the Loop**:
    Explicitly instruct the agent to loop through items and "STOP" to ask for confirmation.

2.  **Propose, Don't Just Do**:
    Frame the interaction as a dialogue: "Should I add X?" rather than "I added X".

3.  **Batch Final Output**:
    Only generate the final file content _after_ the user has confirmed the list of items.

### Example Template

```markdown
## 2. Iterative Configuration (Loop)

**DO NOT** generate the full file immediately. You must propose items **one at a time** to avoid overwhelming the user.

1.  **Pick an Item**: Select one setting or task to propose.
2.  **Propose**: Ask the user: "I recommend adding [Setting X] because [Reason]. Do you agree?"
3.  **Wait**: Wait for user confirmation.
4.  **Repeat**: Continue to the next item.
```

### Benefits

- **Cognitive Load**: Reduces the effort required to review changes.
- **Selective Adoption**: Allows users to easily say "Yes" to some items and "No" to others.
- **Safety**: Prevents accidental overwriting of complex configurations with a "good enough" AI generation.

## Dynamic Context Protocol

### Problem

Relying solely on a single context file (like `AGENTS.md` or a system prompt) limits the AI's depth of understanding. It can lead to outdated information usage or hallucinations if the central index is not exhaustive. Including everything in one file also consumes excessive context window.

### Solution

Implement a "Research Phase" where the AI is explicitly instructed to perform keyword/semantic searches and follow file links to gather specific, relevant details from the knowledge base before starting a task.

### Implementation Steps

1.  **Define Phase**:
    Create a distinct "Research Phase" at the start of the prompt.

2.  **Explicit Instructions**:
    Command the AI to:
    - **Search**: Use tools to find docs (keyword/semantic).
    - **Follow Links**: Traverse from the summary index to detailed files.
    - **Read**: Load the content.
    - **Cross-Reference**: Verify assumptions.

### Example Template

```markdown
### ðŸ” Dynamic Context Protocol (Research Phase)

**Before starting any task, you MUST:**

1.  **Search**: Use your available tools to perform **keyword/regex searches** or **semantic searches** to find specific documentation in `docs/` or `knowledge/` relevant to the user's request.
2.  **Follow Links**: Since this file serves as a summary index, you MUST follow links to obtain detailed information.
3.  **Read**: Load the content of these detailed documents into your context.
4.  **Cross-Reference**: Do NOT rely on assumptions. Always verify against the official documentation found.
```

### Benefits

- **Accuracy**: Ensures the AI uses the most specific and detailed information available.
- **Scalability**: Allows the knowledge base to grow without bloating the primary context file.
- **Autonomy**: Encourages the AI to actively seek information rather than passively relying on pre-loaded context.

## Multilingual Instruction Guardrails

### Problem

When generating content in a target language (e.g., Japanese), LLMs often "summarize" or "localize" technical instructions to make them more natural, inadvertently removing critical constraints or reducing the intensity of commands (e.g., turning "MUST" into "should").

### Solution

Inject a **Critical Translation Rule** immediately before the section that must be preserved. Explicitly forbid summarization and require verbatim translation while preserving structure.

### Implementation Steps

1.  **Identify Critical Sections**: Find instructions that constitute the "Constitution" or "Rules of Engagement" (e.g., security protocols, mandatory steps).
2.  **Inject Warning**: Add a block instructing the model to treat the next section as immutable code, not translatable text.

### Example Template

```markdown
> [!IMPORTANT] > **CRITICAL TRANSLATION RULE**:
> When generating the content below in the **Target Language**, you MUST:
>
> 1. **DO NOT SUMMARIZE**: Translate the text **verbatim**.
> 2. **PRESERVE STRUCTURE**: Keep all structure, bullet points, and warnings.
> 3. **MAINTAIN INTENSITY**: Do not soften "MUST" to "should".
```

### Benefits

- **Consistency**: Ensures agents behave identically regardless of the user's language.
- **Safety**: Prevents the accidental removal of guardrails during translation.
- [ ] Use `[Suggestion]` for non-critical improvements.
- [ ] Maintain a professional tone.

## Few-Shot Chain of Thought (Reasoning)

### Problem

Standard "Chain of Thought" (CoT) instructions ("Think step by step") are often too abstract, leading models to ramble without structure or skip critical reasoning steps.

### Solution

Provide a concrete _example_ of the expected reasoning process within the prompt. This "Few-Shot" approach aligns the model's internal monologue with the desired depth and structure.

### Implementation Steps

1.  **Define Structure**: Create a `<thinking>` block example.
2.  **Show "Good" Pattern**: Demonstrate how to catch an error or weigh options.

### Example Template

```xml
<example_thinking>
User asked: "Refactor the login function."

1.  **Analyze**: I see the `login` function in `auth.ts`. It uses a callback pattern.
2.  **Critique**: Callbacks are outdated. I should use async/await.
3.  **Safety Check**: Are there any existing tests? Yes, `auth.test.ts`. I must ensure they pass.
4.  **Plan**:
    - Update signature to return Promise.
    - Wrap legacy code.
    - Update tests.
</example_thinking>
```

### Benefits

- **Alignment**: The model copies the _logic structure_ of the example.
- **Consistency**: Produces more predictable and high-quality reasoning outputs.

## Explicit Parallelism (Efficiency)

### Problem

Agents often execute search or retrieval tools sequentially (one after another), waiting for each round-trip to complete. This unnecessarily prolongs the "thinking" time and can lead to timeouts or user frustration.

### Solution

Explicitly instruct the agent to run independent tool calls **in parallel** (or in a single batch) whenever possible.

### Implementation Steps

1.  **Identify Independent Tasks**: Look for searches or reads that do not depend on each other.
2.  **Instruction**: Add a "Parallel Execution" rule.

### Example Template

```markdown
## Source Analysis

Perform a **parallel search** strategy:

1. Identify key domain terms.
2. Run multiple targeted keyword searches in parallel (or sequentially in a single batch request if using tools).
3. Do not stop at the first result; gather comprehensive evidence before concluding.
```

### Benefits

- **Speed**: Drastically reduces total execution time.
- **Breadth**: Encourages covering more ground since the "cost" (in time) feels lower to the model.

## Simulation-Based Verification (Perspective Taking & Evidence)

### Problem

When simply asking the AI to "check for consistency" or "review documents," it often performs a superficial pattern match. It may miss deep logical gaps. Furthermore, without explicit evidence logs, the user cannot easily verify _what_ the AI actually checked, forcing them to blindly trust the summary or read verbose internal thought logs.

### Solution

Instruct the AI to **Simulate** a specific scenario from a specific **Perspective** (e.g., "New Developer") and **Record Evidence** for every check. Force it to perform a "Mental Walkthrough" step-by-step. For each step, it must explicitly log the **Input** (what it found), the **Verdict** (Pass/Fail), and the **Evidence** (specific file/line or logic) in the final output.

### Implementation Steps

1.  **Define the Perspective**: Explicitly state who the AI is simulating (e.g., "You are a new developer joining the team").
2.  **Define the Scenario**: logical flow to simulate (e.g., "Adding a new feature").
3.  **Mandate Evidence Logging**: Explicitly instruct the AI to write down _what_ it checked in the final report (e.g., "Checked file X, found Y") rather than just "Checked: OK".
4.  **Step-by-Step Verification**: Ask the AI to list the steps and for each, check Input, Process, and Output.

### Example Template

```markdown
## Simulation: New Feature Implementation

Perform a "Mental Walkthrough" acting as a **New Developer**.
For each step, you MUST log your findings in the report as **Evidence**:

1.  **Requirement Definition**

    - **Simulation**: I am looking for the requirement template.
    - **Check**: Is the output template and save path defined?
    - **Evidence**:
      - Found template in `requirements.prompt.md` at line 15. (âœ… PASS)
      - Save path is defined as `docs/specs/`. (âœ… PASS)

2.  **Architectural Design**
    - **Simulation**: I am trying to run the design skill using the requirement doc.
    - **Check**: Does the skill explicitly reference the Requirement Document as input?
    - **Evidence**:
      - `design.prompt.md` references `{{requirements_files}}` variable. Linkage is valid. (âœ… PASS)

**Report**: Check that each output template and directory is defined. Log specific evidence for every check.
```

### Benefits

- **High Recall**: Finds logic holes that static analysis misses.
- **User Empathy**: Identifies "friction" or frustration points in the process.
- **Auditability**: The user can verify the AI's work by reading the evidence log, building trust without high cognitive load.
